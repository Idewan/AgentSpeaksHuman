{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from prep import Preparation\n",
    "\n",
    "prep = Preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299,299))\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "image_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(image_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../dataset/captions.json\") as jf:\n",
    "    captions = json.loads(jf.read())\n",
    "\n",
    "#Extract lstm from captions data set.\n",
    "lstm_info = captions.pop(\"lstm_labels\")\n",
    "lstm_ranker = lstm_info['labels']\n",
    "max_vocab_size = lstm_info['vocab_size']\n",
    "max_caption_size = lstm_info['longest_caption']\n",
    "\n",
    "#Dataset cleanup\n",
    "img_name_vector = []\n",
    "edited_captions = []\n",
    "\n",
    "image_paths = list(captions.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    captions_image = captions[image_path]\n",
    "    captions_len = len(captions_image)\n",
    "\n",
    "    seed = random.randint(0, captions_len-1)\n",
    "\n",
    "    img_name_vector.append(image_path)\n",
    "    edited_captions.append(f\"<start> {captions_image[seed]} <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(edited_captions[20])\n",
    "Image.open(f\"../dataset/prep_data/{img_name_vector[20]}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize our captions into machine symbols (this is sorted from most to least common)\n",
    "top_v = 45\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_v, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(edited_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "train_seqs = tokenizer.texts_to_sequences(edited_captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "  img_to_cap_vector[img].append(cap)\n",
    "\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "  capt_len = len(img_to_cap_vector[imgt])\n",
    "  img_name_train.extend([imgt])\n",
    "  cap_train.append(img_to_cap_vector[imgt][0])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "  capv_len = len(img_to_cap_vector[imgv])\n",
    "  img_name_val.extend([imgv])\n",
    "  cap_val.append(img_to_cap_vector[imgv][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 512\n",
    "units = 512\n",
    "vocab_size = top_v + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "features_shape = 2048\n",
    "attention_features_shape = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load('../dataset/prep_data/'+img_name.decode('utf-8')+'.jpg.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gru_decoder import GRU_Decoder\n",
    "from cnn_encoder import CNN_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GRU_Decoder(embedding_dim, units, vocab_size)\n",
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  \n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\"\"\" \n",
    "The following two cells and preparation are inspired by https://www.tensorflow.org/tutorials/text/image_captioning#checkpoint. \n",
    "Helped as a support for the rest of the code to understand the original show and tell paper https://arxiv.org/pdf/1502.03044.pdf.\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "  \n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "          \n",
    "          loss += loss_function(target[:,i], predictions)\n",
    "\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  \n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### LISTENER FIXED TRAINING ###############\n",
    "with open(\"../dataset/best_captions.json\", \"r\") as jf:\n",
    "    data = json.loads(jf.read())\n",
    "\n",
    "target_data = data['target_paths']\n",
    "distractor_data = data['distractor_paths']\n",
    "caption_data = data['best_captions']\n",
    "\n",
    "edited_caption_data = []\n",
    "for i in range(len(caption_data)):\n",
    "    edited_caption_data.append(f'<start> {caption_data[i]} <end>')\n",
    "\n",
    "#Split into train, val, and test set.\n",
    "val_slice_index = int(len(target_data)*0.8)\n",
    "test_slice_index = int(len(target_data)*0.9)\n",
    "\n",
    "targ_name_train, targ_name_val, targ_name_test = target_data[:val_slice_index], target_data[val_slice_index:test_slice_index], target_data[test_slice_index:]\n",
    "\n",
    "dis_name_train, dis_name_val, dis_name_test = distractor_data[:val_slice_index], distractor_data[val_slice_index:test_slice_index], distractor_data[test_slice_index:]\n",
    "\n",
    "captions_train, captions_val, captions_test = edited_caption_data[:val_slice_index], edited_caption_data[val_slice_index:test_slice_index], edited_caption_data[test_slice_index:]\n",
    "\n",
    "#Tokenize the captions\n",
    "top_v = 45\n",
    "\n",
    "tokenizer_2 = tf.keras.preprocessing.text.Tokenizer(num_words=top_v,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_2.fit_on_texts(captions_train)\n",
    "tokenizer_2.word_index['<pad>'] = 0\n",
    "tokenizer_2.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer_2.texts_to_sequences(captions_train)\n",
    "\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer_2.texts_to_sequences(captions_train)\n",
    "\n",
    "cap_vector_2 = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "l_dataset = tf.data.Dataset.from_tensor_slices((targ_name_train, dis_name_train, cap_vector_2))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "l_dataset = l_dataset.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "          prep.map_func_oracle, [item1, item2, item3], [tf.float32, tf.float32, tf.int32]))\n",
    "\n",
    "# Shuffle and batch\n",
    "l_dataset = l_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "gru_encoder_l = GRU_Encoder(embedding_dim, units, vocab_size)\n",
    "encoder_l = CNN_Encoder(embedding_dim)\n",
    "\n",
    "optimizer_l = tf.keras.optimizers.Adam()\n",
    "loss_object_l = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# @tf.function\n",
    "def train_step(m, targ, dist):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = gru_encoder_l.reset_state(batch_size=targ.shape[0])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        #could be a serious problem -- most likely! Exciting\n",
    "        features_t = encoder_l(targ)\n",
    "        features_d = encoder_l(dist)\n",
    "\n",
    "        v = gru_encoder_l(m, hidden)\n",
    "        \n",
    "        with tape.stop_recording():\n",
    "            rand_n = random.random()\n",
    "\n",
    "            if rand_n > 0.5:\n",
    "                left = features_t\n",
    "                right = features_d\n",
    "                y_t = tf.convert_to_tensor([[1,0]]*targ.shape[0],dtype=tf.float32)\n",
    "            else:\n",
    "                left = features_d\n",
    "                right = features_t\n",
    "                y_t = tf.convert_to_tensor([[0,1]]*targ.shape[0],dtype=tf.float32)\n",
    "    \n",
    "        x = tf.norm(tf.keras.layers.dot([left, v],axes=2,normalize=True),axis=(1,2))\n",
    "        y = tf.norm(tf.keras.layers.dot([right, v],axes=2,normalize=True),axis=(1,2))\n",
    "        x = tf.reshape(x, (x.shape[0],1))\n",
    "        y = tf.reshape(y, (y.shape[0],1))\n",
    "        z = tf.concat([x,y],axis=1)\n",
    "        y_p = tf.nn.softmax(z)\n",
    "        loss = loss_object_l(y_t, y_p)\n",
    "\n",
    "    trainable_variables = encoder_l.trainable_variables + gru_encoder_l.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer_l.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (targ_tensor, dis_tensor, target)) in enumerate(l_dataset):\n",
    "        target = tf.one_hot(target, vocab_size)\n",
    "        loss = train_step(target, targ_tensor, dis_tensor)\n",
    "        total_loss += loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "            epoch + 1, batch, loss.numpy()))\n",
    "              \n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "      \n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Structural-Only Learning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result, attention_plot = prep.show_result(f\"../dataset/prep_data/{img_name_train[9]}.jpg\")\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "Image.open(f\"../dataset/prep_data/{img_name_train[9]}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result, attention_plot = prep.evaluate(f\"../dataset/custom/dewerpe.jpg\")\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "Image.open(f\"../dataset/custom/dewerpe.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.plot_attention(f\"../dataset/custom/dewerpe.jpg\", result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/best_captions.json\", \"r\") as jf:\n",
    "    data = json.loads(jf.read())\n",
    "\n",
    "target_data = data['target_paths']\n",
    "distractor_data = data['distractor_paths']\n",
    "\n",
    "caption_data = data['best_captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func_test(img_name, img_name_2):\n",
    "  img_tensor = np.load('../dataset/prep_data/'+img_name.decode('utf-8')+'.jpg.npy')\n",
    "  img_tensor_2 = np.load('../dataset/prep_data/'+img_name_2.decode('utf-8')+'.jpg.npy')\n",
    "\n",
    "  return img_tensor, img_tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, val, and test set.\n",
    "val_slice_index = int(len(target_data)*0.8)\n",
    "test_slice_index = int(len(target_data)*0.9)\n",
    "\n",
    "targ_name_train, targ_name_val, targ_name_test = target_data[:val_slice_index], target_data[val_slice_index:test_slice_index], target_data[test_slice_index:]\n",
    "\n",
    "dis_name_train, dis_name_val, dis_name_test = distractor_data[:val_slice_index], distractor_data[val_slice_index:test_slice_index], distractor_data[test_slice_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = tf.data.Dataset.from_tensor_slices((targ_name_train, dis_name_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "testset = testset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func_test, [item1, item2], [tf.float32, tf.float32]))\n",
    "\n",
    "# Shuffle and batch\n",
    "testset = testset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_understanding_captioning(data):\n",
    "    total_right = 0\n",
    "    total_wrong = 0\n",
    "\n",
    "    for (batch, (targ_tensor, dis_tensor)) in enumerate(data):\n",
    "        hidden_s = decoder.reset_state(batch_size=targ_tensor.shape[0])\n",
    "        hidden_l = gru_encoder_l.reset_state(batch_size=targ_tensor.shape[0])\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * targ_tensor.shape[0], 1)\n",
    "\n",
    "        m = dec_input\n",
    "\n",
    "        #Encode the image through CNN\n",
    "        features = encoder(targ_tensor)\n",
    "\n",
    "        for i in range(1,max_length):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden_s)\n",
    "            \n",
    "            preds = tf.nn.softmax(predictions)\n",
    "            indices = tf.math.argmax(predictions, axis=1)\n",
    "            indices = tf.reshape(tf.cast(indices, tf.int32),(targ_tensor.shape[0],1))\n",
    "            \n",
    "            m = tf.concat([m, indices],1)\n",
    "\n",
    "            dec_input = indices\n",
    "\n",
    "        m = tf.one_hot(m, vocab_size)\n",
    "\n",
    "        features_t = encoder_l(targ_tensor)\n",
    "        features_d = encoder_l(dis_tensor)\n",
    "\n",
    "        rand_n = random.random()\n",
    "\n",
    "        left = features_t\n",
    "        right = features_d\n",
    "\n",
    "        v = gru_encoder_l(m, hidden_l)\n",
    "\n",
    "        x = tf.norm(tf.keras.layers.dot([left, v],axes=2,normalize=True),axis=(1,2))\n",
    "        y = tf.norm(tf.keras.layers.dot([right, v],axes=2,normalize=True),axis=(1,2))\n",
    "\n",
    "        mask = tf.math.greater(x, y)\n",
    "        total_right += np.sum(mask.numpy())\n",
    "        total_wrong += np.sum(mask.numpy()==False)\n",
    "\n",
    "    total = total_right + total_wrong\n",
    "\n",
    "    acc = total_right / total\n",
    "\n",
    "    return acc, total_right, total_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc, t_r, t_w = check_understanding_captioning(testset)\n",
    "print(acc, t_r, t_w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func_test(img_name, img_name_2):\n",
    "  img_tensor = np.load('../dataset/prep_data/'+img_name.decode('utf-8')+'.jpg.npy')\n",
    "  img_tensor_2 = np.load('../dataset/prep_data/'+img_name_2.decode('utf-8')+'.jpg.npy')\n",
    "\n",
    "  return img_tensor, img_tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/best_captions.json\", \"r\") as jfec:\n",
    "    data = json.loads(jfec.read())\n",
    "\n",
    "target_data = data['target_paths']\n",
    "distractor_data = data['distractor_paths']\n",
    "\n",
    "val_slice_index = int(len(target_data)*0.8)\n",
    "test_slice_index = int(len(target_data)*0.9)\n",
    "\n",
    "targ_test = target_data[:test_slice_index]\n",
    "\n",
    "dis_test = distractor_data[:test_slice_index]\n",
    "\n",
    "testset = tf.data.Dataset.from_tensor_slices((targ_test, dis_test))\n",
    "testset = testset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func_test, [item1, item2], [tf.float32, tf.float32]))\n",
    "\n",
    "testset = testset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "with open(\"../dataset/easy_captions.json\", \"r\") as jfec:\n",
    "    easy_data = json.loads(jfec.read())\n",
    "\n",
    "target_easy_data = easy_data['target_paths']\n",
    "distractor_easy_data = easy_data['distractor_paths']\n",
    "\n",
    "val_slice_index = int(len(target_easy_data)*0.8)\n",
    "test_slice_index = int(len(target_easy_data)*0.9)\n",
    "\n",
    "targ_easy_test = target_easy_data[:test_slice_index]\n",
    "\n",
    "dis_easy_test = distractor_easy_data[:test_slice_index]\n",
    "\n",
    "testset_easy = tf.data.Dataset.from_tensor_slices((targ_easy_test, dis_easy_test))\n",
    "testset_easy = testset_easy.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func_test, [item1, item2], [tf.float32, tf.float32]))\n",
    "\n",
    "testset_easy = testset_easy.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/best_captions.json\", \"r\") as jf:\n",
    "    data = json.loads(jf.read())\n",
    "    \n",
    "target_data = data['target_paths']\n",
    "distractor_data = data['distractor_paths']\n",
    "caption_data = data['best_captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../human_evaluation/x.json', 'r') as jf:\n",
    "#     X = json.loads(jf.read())   \n",
    "X = np.random.choice(22000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(targ, dis):\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(prep.load_image(targ)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    targ_img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    temp_input = tf.expand_dims(prep.load_image(dis)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    dis_img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(targ_img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 1)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "        dec_input = tf.expand_dims([predicted_id], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "def show_test(num, save):\n",
    "    result = evaluate(f\"../dataset/prep_data/{target_data[num]}.jpg\", f\"../dataset/prep_data/{distractor_data[num]}.jpg\")\n",
    "    if result[len(result)-1] == '<end>':\n",
    "        result = result[:-1]\n",
    "\n",
    "    phrase = f\"Utterance: {' '.join(result)}\"\n",
    "    img_A = mpimg.imread(f\"../dataset/prep_data/{target_data[num]}.jpg\")\n",
    "    img_B = mpimg.imread(f\"../dataset/prep_data/{distractor_data[num]}.jpg\")\n",
    "\n",
    "    rand_n = random.uniform(0,1)\n",
    "\n",
    "    if rand_n < 0.5:\n",
    "        t = 0\n",
    "        d = 1\n",
    "    else:\n",
    "        t = 1\n",
    "        d = 0\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[t].imshow(img_A)\n",
    "    ax[d].imshow(img_B)\n",
    "    fig.suptitle(phrase)\n",
    "    plt.savefig(f'../human_evaluation/captioning/{save}.png')\n",
    "    plt.clf()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = {}\n",
    "for i in tqdm(range(len(X))):\n",
    "    curr_ind = X[i]\n",
    "    t = show_test(curr_ind, i)\n",
    "    correct[i] = t\n",
    "\n",
    "with open(\"../human_evaluation/captioning/correct.json\", \"w\") as wj:\n",
    "    json.dump(correct, wj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
